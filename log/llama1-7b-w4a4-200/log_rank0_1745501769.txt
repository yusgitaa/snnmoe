[2025-04-24 21:36:09 root] (main.py 277): INFO Namespace(a_dynamic_method='per_token', abits=4, act_scales=None, act_shifts=None, addbit=1, alpha=0.75, attn_implementation='eager', aug_loss=False, batch_size=1, cache_dir='./cache', calib_dataset='wikitext2', deactive_amp=False, disable_zero_point=False, epochs=1, eval_ppl=True, group_size=None, let=True, let_lr=0.001, limit=-1, low_p=0.9, lwc=True, lwc_lr=0.01, model='C:/Users/YYK/Desktop/SpikeLLM-main/llama-2-7b', multigpu=False, net=None, nsamples=8, num_fewshot=0, output_dir='./log/llama1-7b-w4a4-200', real_quant=False, resume=None, save_dir=None, seed=2, symmetric=False, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', w_dynamic_method='per_channel', wbits=4, wd=0)
[2025-04-24 21:36:10 root] (main.py 349): INFO === start quantization ===
[2025-04-24 21:36:10 root] (main.py 355): INFO load calibration from ./cache/dataloader_llama_wikitext2_8.cache
[2025-04-24 21:36:10 root] (spike_omniquant.py 114): INFO Starting ...
[2025-04-24 21:36:10 root] (spike_omniquant.py 117): INFO Moving the entire model to GPU for initial processing...
[2025-04-24 21:36:20 root] (spike_omniquant.py 178): INFO Before explicit move - Rotary emb device: cuda:0
[2025-04-24 21:36:20 root] (spike_omniquant.py 181): INFO After explicit move - Rotary emb device: cuda:0
[2025-04-24 21:36:20 root] (spike_omniquant.py 307): INFO === Start quantize layer 0 ===
[2025-04-24 21:36:25 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 0 ---
[2025-04-24 21:36:27 root] (spike_omniquant.py 452): INFO layer 0 iter 0 loss:8.172084926627576e-05 norm:7.860289770178497e-05 max memory_allocated 20834.693359375 
[2025-04-24 21:36:27 root] (spike_omniquant.py 307): INFO === Start quantize layer 1 ===
[2025-04-24 21:36:28 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 1 ---
[2025-04-24 21:36:33 root] (spike_omniquant.py 452): INFO layer 1 iter 0 loss:0.04233241453766823 norm:nan max memory_allocated 20834.693359375 
[2025-04-24 21:36:33 root] (spike_omniquant.py 307): INFO === Start quantize layer 2 ===
[2025-04-24 21:36:34 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 2 ---
[2025-04-24 21:36:38 root] (spike_omniquant.py 452): INFO layer 2 iter 0 loss:0.04291863739490509 norm:0.0004194925131741911 max memory_allocated 20834.693359375 
[2025-04-24 21:36:39 root] (spike_omniquant.py 307): INFO === Start quantize layer 3 ===
[2025-04-24 21:36:39 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 3 ---
[2025-04-24 21:36:44 root] (spike_omniquant.py 452): INFO layer 3 iter 0 loss:0.045872557908296585 norm:0.0006353632779791951 max memory_allocated 20834.693359375 
[2025-04-24 21:36:44 root] (spike_omniquant.py 307): INFO === Start quantize layer 4 ===
[2025-04-24 21:36:45 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 4 ---
[2025-04-24 21:36:49 root] (spike_omniquant.py 452): INFO layer 4 iter 0 loss:0.04973514378070831 norm:0.00044113205512985587 max memory_allocated 20834.693359375 
[2025-04-24 21:36:50 root] (spike_omniquant.py 307): INFO === Start quantize layer 5 ===
[2025-04-24 21:36:50 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 5 ---
[2025-04-24 21:36:55 root] (spike_omniquant.py 452): INFO layer 5 iter 0 loss:0.05467340350151062 norm:0.000383260048693046 max memory_allocated 20834.693359375 
[2025-04-24 21:36:55 root] (spike_omniquant.py 307): INFO === Start quantize layer 6 ===
[2025-04-24 21:36:56 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 6 ---
[2025-04-24 21:37:01 root] (spike_omniquant.py 452): INFO layer 6 iter 0 loss:0.06251604855060577 norm:0.0012370378244668245 max memory_allocated 20834.693359375 
[2025-04-24 21:37:01 root] (spike_omniquant.py 307): INFO === Start quantize layer 7 ===
[2025-04-24 21:37:02 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 7 ---
[2025-04-24 21:37:06 root] (spike_omniquant.py 452): INFO layer 7 iter 0 loss:0.07251355051994324 norm:0.0009585241787135601 max memory_allocated 20834.693359375 
[2025-04-24 21:37:06 root] (spike_omniquant.py 307): INFO === Start quantize layer 8 ===
[2025-04-24 21:37:07 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 8 ---
[2025-04-24 21:37:12 root] (spike_omniquant.py 452): INFO layer 8 iter 0 loss:0.08437871932983398 norm:0.001864052377641201 max memory_allocated 20834.693359375 
[2025-04-24 21:37:12 root] (spike_omniquant.py 307): INFO === Start quantize layer 9 ===
[2025-04-24 21:37:13 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 9 ---
[2025-04-24 21:37:17 root] (spike_omniquant.py 452): INFO layer 9 iter 0 loss:0.09661775827407837 norm:0.0009751638863235712 max memory_allocated 20834.693359375 
[2025-04-24 21:37:18 root] (spike_omniquant.py 307): INFO === Start quantize layer 10 ===
[2025-04-24 21:37:18 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 10 ---
[2025-04-24 21:37:23 root] (spike_omniquant.py 452): INFO layer 10 iter 0 loss:0.11227288097143173 norm:0.0016345351468771696 max memory_allocated 20834.693359375 
[2025-04-24 21:37:23 root] (spike_omniquant.py 307): INFO === Start quantize layer 11 ===
[2025-04-24 21:37:24 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 11 ---
[2025-04-24 21:37:29 root] (spike_omniquant.py 452): INFO layer 11 iter 0 loss:0.12879595160484314 norm:0.003997620195150375 max memory_allocated 20834.693359375 
[2025-04-24 21:37:29 root] (spike_omniquant.py 307): INFO === Start quantize layer 12 ===
[2025-04-24 21:37:30 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 12 ---
[2025-04-24 21:37:34 root] (spike_omniquant.py 452): INFO layer 12 iter 0 loss:0.14615404605865479 norm:0.002265617484226823 max memory_allocated 20834.693359375 
[2025-04-24 21:37:35 root] (spike_omniquant.py 307): INFO === Start quantize layer 13 ===
[2025-04-24 21:37:35 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 13 ---
[2025-04-24 21:37:40 root] (spike_omniquant.py 452): INFO layer 13 iter 0 loss:0.17309916019439697 norm:0.0013541177613660693 max memory_allocated 20834.693359375 
[2025-04-24 21:37:40 root] (spike_omniquant.py 307): INFO === Start quantize layer 14 ===
[2025-04-24 21:37:41 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 14 ---
[2025-04-24 21:37:46 root] (spike_omniquant.py 452): INFO layer 14 iter 0 loss:0.194200336933136 norm:0.0017440211959183216 max memory_allocated 20834.693359375 
[2025-04-24 21:37:46 root] (spike_omniquant.py 307): INFO === Start quantize layer 15 ===
[2025-04-24 21:37:47 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 15 ---
[2025-04-24 21:37:51 root] (spike_omniquant.py 452): INFO layer 15 iter 0 loss:0.2337447702884674 norm:0.0029407446272671223 max memory_allocated 20834.693359375 
[2025-04-24 21:37:52 root] (spike_omniquant.py 307): INFO === Start quantize layer 16 ===
[2025-04-24 21:37:52 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 16 ---
[2025-04-24 21:37:57 root] (spike_omniquant.py 452): INFO layer 16 iter 0 loss:0.30262789130210876 norm:0.002266521332785487 max memory_allocated 20834.693359375 
[2025-04-24 21:37:57 root] (spike_omniquant.py 307): INFO === Start quantize layer 17 ===
[2025-04-24 21:37:58 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 17 ---
[2025-04-24 21:38:02 root] (spike_omniquant.py 452): INFO layer 17 iter 0 loss:0.3552296459674835 norm:0.002451159292832017 max memory_allocated 20834.693359375 
[2025-04-24 21:38:03 root] (spike_omniquant.py 307): INFO === Start quantize layer 18 ===
[2025-04-24 21:38:03 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 18 ---
[2025-04-24 21:38:08 root] (spike_omniquant.py 452): INFO layer 18 iter 0 loss:0.4440333843231201 norm:0.0037551792338490486 max memory_allocated 20834.693359375 
[2025-04-24 21:38:08 root] (spike_omniquant.py 307): INFO === Start quantize layer 19 ===
[2025-04-24 21:38:09 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 19 ---
[2025-04-24 21:38:14 root] (spike_omniquant.py 452): INFO layer 19 iter 0 loss:0.54420405626297 norm:0.0022248844616115093 max memory_allocated 20834.693359375 
[2025-04-24 21:38:14 root] (spike_omniquant.py 307): INFO === Start quantize layer 20 ===
[2025-04-24 21:38:15 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 20 ---
[2025-04-24 21:38:19 root] (spike_omniquant.py 452): INFO layer 20 iter 0 loss:0.6951978206634521 norm:0.0038164665456861258 max memory_allocated 20834.693359375 
[2025-04-24 21:38:20 root] (spike_omniquant.py 307): INFO === Start quantize layer 21 ===
[2025-04-24 21:38:20 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 21 ---
[2025-04-24 21:38:25 root] (spike_omniquant.py 452): INFO layer 21 iter 0 loss:0.8317553997039795 norm:0.001910426770336926 max memory_allocated 20834.693359375 
[2025-04-24 21:38:25 root] (spike_omniquant.py 307): INFO === Start quantize layer 22 ===
[2025-04-24 21:38:26 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 22 ---
[2025-04-24 21:38:30 root] (spike_omniquant.py 452): INFO layer 22 iter 0 loss:1.0295809507369995 norm:0.0039979638531804085 max memory_allocated 20834.693359375 
[2025-04-24 21:38:31 root] (spike_omniquant.py 307): INFO === Start quantize layer 23 ===
[2025-04-24 21:38:31 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 23 ---
[2025-04-24 21:38:36 root] (spike_omniquant.py 452): INFO layer 23 iter 0 loss:1.198222041130066 norm:0.0020667279604822397 max memory_allocated 20834.693359375 
[2025-04-24 21:38:36 root] (spike_omniquant.py 307): INFO === Start quantize layer 24 ===
[2025-04-24 21:38:37 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 24 ---
[2025-04-24 21:38:42 root] (spike_omniquant.py 452): INFO layer 24 iter 0 loss:1.4133415222167969 norm:0.005849091801792383 max memory_allocated 20834.693359375 
[2025-04-24 21:38:42 root] (spike_omniquant.py 307): INFO === Start quantize layer 25 ===
[2025-04-24 21:38:43 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 25 ---
[2025-04-24 21:38:47 root] (spike_omniquant.py 452): INFO layer 25 iter 0 loss:1.5895148515701294 norm:0.004138723015785217 max memory_allocated 20834.693359375 
[2025-04-24 21:38:47 root] (spike_omniquant.py 307): INFO === Start quantize layer 26 ===
[2025-04-24 21:38:48 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 26 ---
[2025-04-24 21:38:53 root] (spike_omniquant.py 452): INFO layer 26 iter 0 loss:1.8610244989395142 norm:0.0049097565934062 max memory_allocated 20834.693359375 
[2025-04-24 21:38:53 root] (spike_omniquant.py 307): INFO === Start quantize layer 27 ===
[2025-04-24 21:38:54 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 27 ---
[2025-04-24 21:38:58 root] (spike_omniquant.py 452): INFO layer 27 iter 0 loss:2.109194278717041 norm:0.002818917855620384 max memory_allocated 20834.693359375 
[2025-04-24 21:38:59 root] (spike_omniquant.py 307): INFO === Start quantize layer 28 ===
[2025-04-24 21:38:59 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 28 ---
[2025-04-24 21:39:04 root] (spike_omniquant.py 452): INFO layer 28 iter 0 loss:2.4398210048675537 norm:0.004917690064758062 max memory_allocated 20834.693359375 
[2025-04-24 21:39:04 root] (spike_omniquant.py 307): INFO === Start quantize layer 29 ===
[2025-04-24 21:39:05 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 29 ---
[2025-04-24 21:39:10 root] (spike_omniquant.py 452): INFO layer 29 iter 0 loss:2.839869260787964 norm:0.007354829926043749 max memory_allocated 20834.693359375 
[2025-04-24 21:39:10 root] (spike_omniquant.py 307): INFO === Start quantize layer 30 ===
[2025-04-24 21:39:10 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 30 ---
[2025-04-24 21:39:15 root] (spike_omniquant.py 452): INFO layer 30 iter 0 loss:4.522049427032471 norm:nan max memory_allocated 20834.693359375 
[2025-04-24 21:39:15 root] (spike_omniquant.py 307): INFO === Start quantize layer 31 ===
[2025-04-24 21:39:16 root] (spike_omniquant.py 387): INFO --- Finished static calibration for layer 31 ---
[2025-04-24 21:39:21 root] (spike_omniquant.py 452): INFO layer 31 iter 0 loss:15.492868423461914 norm:nan max memory_allocated 20834.693359375 
[2025-04-24 21:39:21 root] (main.py 378): INFO 191.27444005012512
[2025-04-24 21:39:23 root] (main.py 113): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-04-24 21:40:51 root] (main.py 157): INFO wikitext2 : 60.54280471801758
[2025-04-24 21:40:51 root] (main.py 113): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-04-24 21:43:08 root] (main.py 157): INFO c4 : 65.25450897216797
[2025-04-25 00:34:32 root] (main.py 168): INFO {'seed': 2, 'wikitext2': 60.54280471801758, 'c4': 65.25450897216797, 'results': {'boolq': {'acc': 0.5131498470948013, 'acc_stderr': 0.008742030090044971}, 'hellaswag': {'acc': 0.29874526986656047, 'acc_stderr': 0.004567724872057201, 'acc_norm': 0.3467436765584545, 'acc_norm_stderr': 0.00474960619636335}, 'arc_easy': {'acc': 0.3291245791245791, 'acc_stderr': 0.009642048058060975, 'acc_norm': 0.31523569023569026, 'acc_norm_stderr': 0.009533589368505848}, 'arc_challenge': {'acc': 0.22525597269624573, 'acc_stderr': 0.012207839995407312, 'acc_norm': 0.26621160409556316, 'acc_norm_stderr': 0.012915774781523224}, 'piqa': {'acc': 0.5767138193688792, 'acc_stderr': 0.01152769947361448, 'acc_norm': 0.5680087051142546, 'acc_norm_stderr': 0.011557407210100255}, 'winogrande': {'acc': 0.5280189423835833, 'acc_stderr': 0.014030404213405786}}, 'versions': {'boolq': 1, 'hellaswag': 0, 'arc_easy': 0, 'arc_challenge': 0, 'piqa': 0, 'winogrande': 0}, 'config': {'model': <models.LMClass.LMClass object at 0x000001BA474E11C0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
